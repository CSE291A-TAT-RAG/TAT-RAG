# ============================================
# TAT-RAG Configuration
# ============================================

# Qdrant Vector Database
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=documents

# ============================================
# LLM Configuration
# ============================================
# Provider: 'openai' or 'ollama'
LLM_PROVIDER=ollama
LLM_MODEL=qwen3:8b
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000

# Ollama Configuration (if using ollama)
OLLAMA_BASE_URL=http://host.docker.internal:11434

# OpenAI Configuration (if using openai)
# OPENAI_API_KEY=your_openai_api_key_here

# ============================================
# Embedding Configuration
# ============================================
# Provider: 'local' or 'openai'
EMBEDDING_PROVIDER=local
EMBEDDING_MODEL=BAAI/bge-m3
EMBEDDING_BATCH_SIZE=100

# Device for local embeddings: 'cuda', 'cpu', or leave empty for auto
# EMBEDDING_DEVICE=cuda

# Local model cache directory (relative to project root or absolute path)
# If not set, uses default HuggingFace cache (~/.cache/huggingface/)
EMBEDDING_CACHE_DIR=./models

# OpenAI Embedding (if using openai provider)
# EMBEDDING_MODEL=text-embedding-ada-002
# OPENAI_API_KEY=your_openai_api_key_here

# ============================================
# RAG Pipeline Configuration
# ============================================
RAG_TOP_K=5
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# ============================================
# Alternative Configurations
# ============================================

# Full Local Mode (default):
# LLM_PROVIDER=ollama
# LLM_MODEL=qwen2.5:8b
# EMBEDDING_PROVIDER=local
# EMBEDDING_MODEL=BAAI/bge-m3

# Cloud Mode (OpenAI):
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-3.5-turbo
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-ada-002
# OPENAI_API_KEY=your_key_here

# Hybrid Mode (Local Embedding + Cloud LLM):
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4
# EMBEDDING_PROVIDER=local
# EMBEDDING_MODEL=BAAI/bge-m3
# OPENAI_API_KEY=your_key_here
