# ============================================
# TAT-RAG Configuration Template
# ============================================
# SETUP: cp .env.example .env
# Then edit .env to configure your environment

# ============================================
# Qdrant Vector Database
# ============================================
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=documents

# ============================================
# LLM Configuration
# ============================================
# Choose 'ollama' for local or 'bedrock' for AWS cloud
# LLM_PROVIDER=ollama
# LLM_MODEL=qwen3:8b
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000

# LLM_PROVIDER=bedrock
# LLM_MODEL=anthropic.claude-3-haiku-20240307-v1:0

# LLM_PROVIDER=gemini
# LLM_MODEL=gemini-2.0-flash-lite
# GEMINI_API_KEY=
# LLM_REQUEST_INTERVAL=3

# --- Ollama Configuration (Local) ---
OLLAMA_BASE_URL=http://host.docker.internal:11434

# --- AWS Bedrock Configuration (Cloud) ---
# Active when LLM_PROVIDER=bedrock
# Uncomment and fill these lines if using Bedrock:
# AWS_REGION=us-west-2
# AWS_ACCESS_KEY_ID=ASIA2GORDRDS4OBZIFD2
# AWS_SECRET_ACCESS_KEY=6tTav02Rj5LfSmhObz7fdyi9JgcReVco9ppP+rJt
# AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEAYaCXVzLXdlc3QtMiJIMEYCIQDM3M7dWLzCkiug0rlM/vO78sgORoTKnda5pNZ/yhhGAgIhAI4rgB9g1Bsyp5X4Z8GIbTVDxkENVbkhpGH9nczkDqFXKp0CCK///////////wEQABoMNzAxMDU1MDc2NTgxIgzMwoEXTLTGryo59h4q8QEFbXv+CDq+QGjVkMEaRNQKivIxgqQlk6D2193X9FVBCewu+lztQFh20I1wS4QOKxyLIGo7OugBsOqcu/XTdebSREgvQzC0wfeTbs2cFwVT0HVDp20Q510SRbAHqpa0KHc16VSTttuRLA71K4RCV5xLvv7aAP8ua2HfTnU0q1hUtwpQNPyrtBS2pTf/eCvl7ugG7uCwr+4xjeCDofjt1xk5khGp2XTOV0mGn/o4kSddBhO63QUoBJi62F9blksIW1canRM0CHmhyKnqK5H8JSt+JYLXzZo3So4GJ1dZ38CIW3eie8bf9a4YoFhcv5NYJaKPMMbqyscGOpwBAAuue0KpyFTDA876wpUQXONu5ugqVqrdbdmYdhE+mI7vBukjWAhJBsRux7ZCZiS9uqayMHjFXNWJQ5G3toMZii+PSl3RX+U1YQ17et9OhA/R4Pcu53C1jyhp3Mp5+Ki2VHdVSJUc7tZiy6j1uHqc3rIk+y9ATPxC3AwQ7hkVd4sROmtRfd2oexgdR7qnuvuMgPG0UsSBvyxp6X48
# ============================================
# Embedding Configuration
# ============================================
EMBEDDING_PROVIDER=local
EMBEDDING_MODEL=BAAI/bge-m3
EMBEDDING_BATCH_SIZE=512
EMBEDDING_DEVICE=cuda
EMBEDDING_CACHE_DIR=./models

# ============================================
# RAG Pipeline Configuration
# ============================================
# Maximum number of documents to retrieve
RAG_TOP_K=5

# Minimum similarity score (0.0 - 1.0)
# Only return documents with score >= threshold
# Higher = stricter (more relevant), Lower = looser (more results)
# Recommended: 0.3-0.5 for balanced quality/quantity
RAG_SCORE_THRESHOLD=0.5

CHUNK_SIZE=1000
CHUNK_OVERLAP=200


### ============================================
# docker-compose down
# docker-compose up -d
