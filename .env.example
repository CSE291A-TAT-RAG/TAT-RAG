# ============================================
# TAT-RAG Configuration Template
# ============================================
# SETUP: cp .env.example .env
# Then edit .env to configure your environment

# ============================================
# Qdrant Vector Database
# ============================================
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=documents

# ============================================
# LLM Configuration
# ============================================
# Choose 'ollama' for local or 'bedrock' for AWS cloud
LLM_PROVIDER=bedrock
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000

# --- Model Selection ---
# For Ollama (local): Use models like qwen3:8b, llama3:8b
# LLM_MODEL=qwen3:8b

# For AWS Bedrock: Use Claude models for full Ragas evaluation support
# Recommended: Claude 3.5 Sonnet (widely available)
LLM_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0
# Alternatives:
# LLM_MODEL=anthropic.claude-3-5-sonnet-20240620-v1:0
# LLM_MODEL=anthropic.claude-3-sonnet-20240229-v1:0

# --- Ollama Configuration (Local) ---
OLLAMA_BASE_URL=http://host.docker.internal:11434

# --- AWS Bedrock Configuration (Cloud) ---
# Active when LLM_PROVIDER=bedrock
# Uncomment and fill these lines if using Bedrock:
# AWS_REGION=us-west-2
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_SESSION_TOKEN=your_session_token  # Optional, for temporary credentials

# ============================================
# Embedding Configuration
# ============================================
EMBEDDING_PROVIDER=local
EMBEDDING_MODEL=BAAI/bge-m3
EMBEDDING_BATCH_SIZE=100
EMBEDDING_DEVICE=cuda
EMBEDDING_CACHE_DIR=./models

# ============================================
# RAG Pipeline Configuration
# ============================================
RAG_TOP_K=5
CHUNK_SIZE=1000
CHUNK_OVERLAP=200


### ============================================
# docker-compose down
# docker-compose up -d