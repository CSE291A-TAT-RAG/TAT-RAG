{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d359e888",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58492ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import re, hashlib\n",
    "\n",
    "import fitz\n",
    "import pdfplumber\n",
    "\n",
    "# for table extraction\n",
    "try:\n",
    "    import camelot  # for tables (lattice/stream)\n",
    "    _has_camelot = True\n",
    "except Exception:\n",
    "    _has_camelot = False\n",
    "\n",
    "# tokenizer\n",
    "try:\n",
    "    import tiktoken\n",
    "    _enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    def tok_len(s: str) -> int: return len(_enc.encode(s))\n",
    "    def tok_split(s: str, n: int) -> List[str]:\n",
    "        ids = _enc.encode(s)\n",
    "        return [_enc.decode(ids[i:i+n]) for i in range(0, len(ids), n)]\n",
    "except Exception:\n",
    "    def tok_len(s: str) -> int: return max(1, len(s)//4)\n",
    "    def tok_split(s: str, n: int) -> List[str]:\n",
    "        step = n*4\n",
    "        return [s[i:i+step] for i in range(0, len(s), step)]\n",
    "\n",
    "def md5(s: str) -> str: return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "@dataclass\n",
    "class ParagraphBlock:\n",
    "    text: str\n",
    "    page: int\n",
    "    section_path: str\n",
    "\n",
    "@dataclass\n",
    "class TableBlock:\n",
    "    rows: List[List[str]]  # rows[0] is table header\n",
    "    page: int\n",
    "    title: str\n",
    "    currency: Optional[str] = None\n",
    "    unit: Optional[str] = None\n",
    "\n",
    "HEADING_PATTERNS = [\n",
    "    r\"^Item\\s+1A?\\b.*\",  # Item 1 / 1A Risk Factors\n",
    "    r\"^Item\\s+7\\b.*\",    # Item 7 MD&A\n",
    "    r\"^Item\\s+7A\\b.*\",\n",
    "    r\"^Item\\s+8\\b.*\",    # Financial Statements and Supplementary Data\n",
    "    r\"^Management.?s Discussion.*\",\n",
    "    r\"^Consolidated\\s+(Statements?|Balance Sheets?|Cash Flows?).*\",\n",
    "    r\"^Notes?\\s+to\\s+Consolidated\\s+Financial\\s+Statements.*\",\n",
    "    r\"^Risk\\s+Factors.*\",\n",
    "] # TODO\n",
    "HEADING_RE = re.compile(\"|\".join(HEADING_PATTERNS), re.I)\n",
    "\n",
    "CURRENCY_HINT = re.compile(r\"\\b(USD|US\\$|\\$|HKD|EUR|GBP|RMB|CNY)\\b\")\n",
    "UNIT_HINT = re.compile(r\"\\b(thousands|millions|billions|â€™000|000s)\\b\", re.I)\n",
    "\n",
    "def extract_paragraphs_with_sections(pdf_path: str) -> List[ParagraphBlock]:\n",
    "    blocks: List[ParagraphBlock] = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    current_section = \"Front\"\n",
    "    for pno in range(len(doc)):\n",
    "        page = doc[pno]\n",
    "        text = page.get_text(\"text\")\n",
    "        chunks = [c.strip() for c in re.split(r\"\\n\\s*\\n\", text) if c.strip()]\n",
    "        for c in chunks:\n",
    "            lines = c.splitlines()\n",
    "            head = lines[0].strip()\n",
    "            if HEADING_RE.match(head):\n",
    "                current_section = head\n",
    "            blocks.append(ParagraphBlock(text=c, page=pno+1, section_path=current_section))\n",
    "    doc.close()\n",
    "    return blocks\n",
    "\n",
    "# TODO: check the extraction quality\n",
    "def extract_tables(pdf_path: str) -> List[TableBlock]:\n",
    "    tbls: List[TableBlock] = []\n",
    "    if _has_camelot:\n",
    "        try:\n",
    "            t1 = camelot.read_pdf(pdf_path, flavor=\"lattice\", pages=\"all\")\n",
    "            for t in t1:\n",
    "                rows = [list(map(str, r)) for r in t.df.values.tolist()]\n",
    "                if rows:\n",
    "                    title = rows[0][0].strip() if rows[0] and len(rows[0][0]) < 120 else \"Table\"\n",
    "                    meta = \" \".join(sum(rows[:3], []))[:500]\n",
    "                    currency = (CURRENCY_HINT.search(meta) or [None]) and (CURRENCY_HINT.search(meta).group(0) if CURRENCY_HINT.search(meta) else None)\n",
    "                    unit = (UNIT_HINT.search(meta) or [None]) and (UNIT_HINT.search(meta).group(0) if UNIT_HINT.search(meta) else None)\n",
    "                    tbls.append(TableBlock(rows=rows, page=t.page, title=title, currency=currency, unit=unit))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not tbls:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for pno, page in enumerate(pdf.pages, start=1):\n",
    "                try:\n",
    "                    tables = page.extract_tables()\n",
    "                    for rows in tables:\n",
    "                        rows = [[(cell or \"\").strip() for cell in row] for row in rows if any(cell for cell in row)]\n",
    "                        if rows:\n",
    "                            meta = \" \".join(sum(rows[:2], []))[:500]\n",
    "                            currency = (CURRENCY_HINT.search(meta).group(0) if CURRENCY_HINT.search(meta) else None)\n",
    "                            unit = (UNIT_HINT.search(meta).group(0) if UNIT_HINT.search(meta) else None)\n",
    "                            tbls.append(TableBlock(rows=rows, page=pno, title=\"Table\", currency=currency, unit=unit))\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return tbls\n",
    "\n",
    "def chunk_paragraph(text: str, size: int = 640, overlap: int = 96) -> List[str]:\n",
    "    if tok_len(text) <= size:\n",
    "        return [text]\n",
    "    step = max(1, size - overlap)\n",
    "    parts = tok_split(text, step)\n",
    "    res = []\n",
    "    prev_tail = \"\"\n",
    "    for i, p in enumerate(parts):\n",
    "        cur = (prev_tail + p) if i > 0 else p\n",
    "        if tok_len(cur) > size:\n",
    "            cur = tok_split(cur, size)[0]\n",
    "        res.append(cur)\n",
    "        tail_tokens = tok_split(cur, max(1, tok_len(cur)-overlap))\n",
    "        prev_tail = tail_tokens[-1] if tail_tokens else \"\"\n",
    "    return res\n",
    "\n",
    "def chunk_table_by_rows(rows: List[List[str]], window: int = 4) -> List[List[List[str]]]:\n",
    "    if not rows:\n",
    "        return []\n",
    "    header = rows[0]\n",
    "    data = rows[1:]\n",
    "    if not data:\n",
    "        return [rows]\n",
    "    chunks = []\n",
    "    for i in range(0, len(data), window):\n",
    "        block = [header] + data[i:i+window]\n",
    "        chunks.append(block)\n",
    "    return chunks\n",
    "\n",
    "def build_chunks_for_financial_report(\n",
    "    pdf_path: str,\n",
    "    doc_meta: Dict,\n",
    "    para_size: int = 640,\n",
    "    para_overlap: int = 96,\n",
    "    table_window: int = 4\n",
    ") -> List[Dict]:\n",
    "    out: List[Dict] = []\n",
    "    doc_id = doc_meta.get(\"doc_id\") or md5(pdf_path)\n",
    "    paras = extract_paragraphs_with_sections(pdf_path)\n",
    "    order = 0\n",
    "    for pb in paras:\n",
    "        splits = chunk_paragraph(pb.text, para_size, para_overlap)\n",
    "        for j, s in enumerate(splits):\n",
    "            md = {\n",
    "                **doc_meta,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"type\": \"paragraph\",\n",
    "                \"section_path\": pb.section_path,\n",
    "                \"page\": pb.page,\n",
    "                \"order\": order,\n",
    "                \"inner_index\": j,\n",
    "                \"tokens\": tok_len(s),\n",
    "            }\n",
    "            md[\"hash\"] = md5(f\"{doc_id}|p|{pb.page}|{order}|{j}|{s[:80]}\")\n",
    "            out.append({\"text\": s, \"metadata\": md})\n",
    "        order += 1\n",
    "\n",
    "    tables = extract_tables(pdf_path)\n",
    "    for t_idx, tb in enumerate(tables):\n",
    "        t_chunks = chunk_table_by_rows(tb.rows, window=table_window)\n",
    "        for k, block in enumerate(t_chunks):\n",
    "            tsv = \"\\n\".join([\"\\t\".join(row) for row in block])\n",
    "            md = {\n",
    "                **doc_meta,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"type\": \"table\",\n",
    "                \"section_path\": f\"{tb.title}\",\n",
    "                \"page\": tb.page,\n",
    "                \"table_id\": f\"T{t_idx}\",\n",
    "                \"table_chunk\": k,\n",
    "                \"headers\": block[0],\n",
    "                \"currency\": tb.currency,\n",
    "                \"unit\": tb.unit,\n",
    "                \"tokens\": tok_len(tsv),\n",
    "            }\n",
    "            md[\"headers_hash\"] = md5(\"|\".join(md[\"headers\"]))\n",
    "            md[\"hash\"] = md5(f\"{doc_id}|t|{tb.page}|{t_idx}|{k}|{tsv[:80]}\")\n",
    "            out.append({\"text\": tsv, \"metadata\": md})\n",
    "    return out\n",
    "\n",
    "# example usage\n",
    "data_folder = \"./tat_docs/\"\n",
    "chunks = build_chunks_for_financial_report(\n",
    "    f\"{data_folder}/adobe-systems-inc_2019.pdf\",\n",
    "    {\"doc_id\": \"adobe-systems-inc_2019\"} # can add more metadata here\n",
    ")\n",
    "print(len(chunks), chunks[0][\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ec5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# remind to change the path to your local path\n",
    "def get_files(base='./tat_docs'):\n",
    "    for path, dir_list, file_list in os.walk(base):\n",
    "        for file_name in file_list:\n",
    "            if file_name.endswith('.pdf'):\n",
    "                yield os.path.join(path, file_name)\n",
    "\n",
    "total_chunks = []\n",
    "for f in get_files():\n",
    "    print(f\"Processing {f} ...\")\n",
    "    cks = build_chunks_for_financial_report(\n",
    "        f,\n",
    "        {\"doc_id\": f.split('/')[-1]}\n",
    "    )\n",
    "    total_chunks.extend(cks)\n",
    "print(f\"Total {len(total_chunks)} chunks from {len(list(get_files()))} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e3a66",
   "metadata": {},
   "source": [
    "### Test for table extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06017156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot, pdfplumber, tabula\n",
    "from pathlib import Path\n",
    "import tempfile, re\n",
    "\n",
    "def extract_with_camelot(pdf_path, flavor, pages=\"1\", **kwargs):\n",
    "    try:\n",
    "        tables = camelot.read_pdf(pdf_path, flavor=flavor, pages=pages, **kwargs)\n",
    "        return [t.df for t in tables]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_with_tabula(pdf_path, pages=\"1\", lattice=False, stream=False, area=None, columns=None):\n",
    "    try:\n",
    "        opts = {}\n",
    "        if area: opts[\"area\"] = area\n",
    "        if columns: opts[\"columns\"] = columns\n",
    "        dfs = tabula.read_pdf(pdf_path, pages=pages, lattice=lattice, stream=stream, guess=(not (area or columns)), multiple_tables=True, **opts)\n",
    "        return dfs or []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_with_pdfplumber(pdf_path, page=1):\n",
    "    out = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        tb = pdf.pages[page-1].extract_tables()\n",
    "        for rows in tb:\n",
    "            clean = [[(c or \"\").replace(\"\\n\", \" \").strip() for c in row] for row in rows]\n",
    "            out.append(clean)\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymusic",
   "language": "python",
   "name": "pymusic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
